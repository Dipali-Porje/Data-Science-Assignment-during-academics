{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Problem statement : Text Analytics \n",
    "1. Extract Sample document and apply following document preprocessing methods: \n",
    "Tokenization, POS Tagging, stop words removal, Stemming and Lemmatization. \n",
    "2. Create representation of documents by calculating Term Frequency and Inverse \n",
    "DocumentFrequency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3033,
     "status": "ok",
     "timestamp": 1740090531724,
     "user": {
      "displayName": "dipali porje",
      "userId": "15697648671118973123"
     },
     "user_tz": -330
    },
    "id": "Vnl4LowWa4WN",
    "outputId": "8c53efe7-e928-448a-9a38-c3587ffa8ca7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (3.9.1)\n",
      "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk) (8.1.8)\n",
      "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk) (2024.11.6)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk) (4.67.1)\n"
     ]
    }
   ],
   "source": [
    "pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 17,
     "status": "ok",
     "timestamp": 1740090853270,
     "user": {
      "displayName": "dipali porje",
      "userId": "15697648671118973123"
     },
     "user_tz": -330
    },
    "id": "XvwVaxaab5_F",
    "outputId": "91a19d4f-8fc5-4190-b2bf-9f11736d36b7"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Error loading averaged_perception_tagger: Package\n",
      "[nltk_data]     'averaged_perception_tagger' not found in index\n",
      "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')   #punkt is used for tokenization, dividing a text into words or sentences.\n",
    "nltk.download('averaged_perception_tagger')   #function is used for Part-Of-Speech (POS) tagging, which helps to identify whether a word is a noun, verb, adjective, etc.\n",
    "nltk.download(\"wordnet\")    #wordnet is a lexical database used for word meanings and relationships\n",
    "nltk.download('stopwords')   #stopwords are common words like \"is\", \"and\", \"the\" that are usually ignored in text processing.\n",
    "nltk.download('punkt_tab')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1740091927422,
     "user": {
      "displayName": "dipali porje",
      "userId": "15697648671118973123"
     },
     "user_tz": -330
    },
    "id": "Mwjm_8ZGcRgz"
   },
   "outputs": [],
   "source": [
    "from nltk import sent_tokenize\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tag import pos_tag\n",
    "from nltk.corpus import wordnet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3dHZmF8mkzb6"
   },
   "source": [
    "Perform tokenization on document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1740091928567,
     "user": {
      "displayName": "dipali porje",
      "userId": "15697648671118973123"
     },
     "user_tz": -330
    },
    "id": "XBa0_55RffbS",
    "outputId": "527c8d17-e6ce-486c-d219-bb942291e38d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens: ['NLP', 'is', 'a', 'field', 'of', 'AI', 'or', 'subset', 'of', 'AI', '.', 'NLTK', 'is', 'a', 'powerful', 'tool', 'for', 'text', 'analysis', '.', 'NLP', 'plays', 'crucial', 'role', 'in', 'various', 'tasks', 'such', 'as', 'building', 'a', 'chatgpt', 'etc', '.']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Function to tokenize a document\n",
    "def tokenize_text(document):\n",
    "    return word_tokenize(document)    # breaks down the input document (a string of text) into individual words or tokens.\n",
    "\n",
    "# Example Usage:\n",
    "document = \"NLP is a field of AI or subset of AI. NLTK is a powerful tool for text analysis. NLP plays crucial role in various tasks such as building a chatgpt etc.\"\n",
    "tokens = tokenize_text(document)\n",
    "print(\"Tokens:\", tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 101,
     "status": "ok",
     "timestamp": 1740092141662,
     "user": {
      "displayName": "dipali porje",
      "userId": "15697648671118973123"
     },
     "user_tz": -330
    },
    "id": "QnggvdJZkhTh",
    "outputId": "9421e125-5b30-49f6-d583-7e7a8d397dbf"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
      "[nltk_data]     /root/nltk_data...\n",
      "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger_eng.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "# Correct download for the averaged perceptron tagger\n",
    "nltk.download('averaged_perceptron_tagger_eng')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 245,
     "status": "ok",
     "timestamp": 1740092143801,
     "user": {
      "displayName": "dipali porje",
      "userId": "15697648671118973123"
     },
     "user_tz": -330
    },
    "id": "s93QFmyEkpmh",
    "outputId": "74b44bc0-8fa7-458a-e2ad-8b3ecbedb5df"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('NLP', 'NNP'), ('is', 'VBZ'), ('a', 'DT'), ('field', 'NN'), ('of', 'IN'), ('AI', 'NNP'), ('or', 'CC'), ('subset', 'NN'), ('of', 'IN'), ('AI', 'NNP'), ('.', '.'), ('NLTK', 'NNP'), ('is', 'VBZ'), ('a', 'DT'), ('powerful', 'JJ'), ('tool', 'NN'), ('for', 'IN'), ('text', 'JJ'), ('analysis', 'NN'), ('.', '.'), ('NLP', 'NNP'), ('plays', 'VBZ'), ('crucial', 'JJ'), ('role', 'NN'), ('in', 'IN'), ('various', 'JJ'), ('tasks', 'NNS'), ('such', 'JJ'), ('as', 'IN'), ('building', 'VBG'), ('a', 'DT'), ('chatgpt', 'NN'), ('etc', 'NN'), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "# Apply POS tagging\n",
    "tagged_tokens = nltk.pos_tag(tokens)  # POS tagging identifies the grammatical category of each word in a sentence (e.g., noun, verb, adjective)\n",
    "\n",
    "# Print the tagged tokens\n",
    "print(tagged_tokens)   # returns list of tuples where each tuple contains a token and its corresponding POS tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 17,
     "status": "ok",
     "timestamp": 1740091321630,
     "user": {
      "displayName": "dipali porje",
      "userId": "15697648671118973123"
     },
     "user_tz": -330
    },
    "id": "rOyzBvwNg5Y6",
    "outputId": "ff12c236-b0ad-4da8-d8fb-3e241e4a8852"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered Tokens (after stopwords removal): ['NLP', 'field', 'AI', 'subset', 'AI', '.', 'NLTK', 'powerful', 'tool', 'text', 'analysis', '.', 'NLP', 'plays', 'crucial', 'role', 'various', 'tasks', 'building', 'chatgpt', 'etc', '.']\n",
      " \n",
      "\n",
      "Stop words are :  ['a', 'about', 'above', 'after', 'again', 'against', 'ain', 'all', 'am', 'an', 'and', 'any', 'are', 'aren', \"aren't\", 'as', 'at', 'be', 'because', 'been', 'before', 'being', 'below', 'between', 'both', 'but', 'by', 'can', 'couldn', \"couldn't\", 'd', 'did', 'didn', \"didn't\", 'do', 'does', 'doesn', \"doesn't\", 'doing', 'don', \"don't\", 'down', 'during', 'each', 'few', 'for', 'from', 'further', 'had', 'hadn', \"hadn't\", 'has', 'hasn', \"hasn't\", 'have', 'haven', \"haven't\", 'having', 'he', \"he'd\", \"he'll\", 'her', 'here', 'hers', 'herself', \"he's\", 'him', 'himself', 'his', 'how', 'i', \"i'd\", 'if', \"i'll\", \"i'm\", 'in', 'into', 'is', 'isn', \"isn't\", 'it', \"it'd\", \"it'll\", \"it's\", 'its', 'itself', \"i've\", 'just', 'll', 'm', 'ma', 'me', 'mightn', \"mightn't\", 'more', 'most', 'mustn', \"mustn't\", 'my', 'myself', 'needn', \"needn't\", 'no', 'nor', 'not', 'now', 'o', 'of', 'off', 'on', 'once', 'only', 'or', 'other', 'our', 'ours', 'ourselves', 'out', 'over', 'own', 're', 's', 'same', 'shan', \"shan't\", 'she', \"she'd\", \"she'll\", \"she's\", 'should', 'shouldn', \"shouldn't\", \"should've\", 'so', 'some', 'such', 't', 'than', 'that', \"that'll\", 'the', 'their', 'theirs', 'them', 'themselves', 'then', 'there', 'these', 'they', \"they'd\", \"they'll\", \"they're\", \"they've\", 'this', 'those', 'through', 'to', 'too', 'under', 'until', 'up', 've', 'very', 'was', 'wasn', \"wasn't\", 'we', \"we'd\", \"we'll\", \"we're\", 'were', 'weren', \"weren't\", \"we've\", 'what', 'when', 'where', 'which', 'while', 'who', 'whom', 'why', 'will', 'with', 'won', \"won't\", 'wouldn', \"wouldn't\", 'y', 'you', \"you'd\", \"you'll\", 'your', \"you're\", 'yours', 'yourself', 'yourselves', \"you've\"]\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Function to remove stopwords from the token list\n",
    "def remove_stopwords(tokens):\n",
    "    stop_words = set(stopwords.words('english'))     #retrieves a list of common stopwords for the English language. Then set() operation convertes list into a set for faster lookups.\n",
    "    return [word for word in tokens if word.lower() not in stop_words]   #list comprehension, a concise way to create a new list by iterating over the tokens and filtering out words that are in the stop_words set.\n",
    "\n",
    "# Example Usage:\n",
    "filtered_tokens = remove_stopwords(tokens)   # removes stopwords (common words like \"the\", \"is\", \"in\", etc.) from a list of tokens.\n",
    "print(\"Filtered Tokens (after stopwords removal):\", filtered_tokens)\n",
    "print( ' \\n\\nStop words are : ', stopwords.words('english'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 44,
     "status": "ok",
     "timestamp": 1740091344861,
     "user": {
      "displayName": "dipali porje",
      "userId": "15697648671118973123"
     },
     "user_tz": -330
    },
    "id": "oBSEv7YThFva",
    "outputId": "807b1759-1032-4819-de82-325af1a819b1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stemmed Tokens: ['nlp', 'field', 'ai', 'subset', 'ai', '.', 'nltk', 'power', 'tool', 'text', 'analysi', '.', 'nlp', 'play', 'crucial', 'role', 'variou', 'task', 'build', 'chatgpt', 'etc', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "#Stemming  reduces a word to its root or base form, often by stripping off prefixes or suffixes.\n",
    "\n",
    "# Function to apply stemming to a list of tokens\n",
    "def stemming(tokens):\n",
    "    stemmer = PorterStemmer()    #PorterStemmer is commonly used algorithm for stemming in NLP.Creating object of PorterStemmer\n",
    "    return [stemmer.stem(word) for word in tokens]\n",
    "\n",
    "# Example Usage:\n",
    "stemmed_tokens = stemming(filtered_tokens)\n",
    "print(\"Stemmed Tokens:\", stemmed_tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3946,
     "status": "ok",
     "timestamp": 1740091382271,
     "user": {
      "displayName": "dipali porje",
      "userId": "15697648671118973123"
     },
     "user_tz": -330
    },
    "id": "Fm-quG1RhsZZ",
    "outputId": "f078c9a5-d868-4a7d-ef07-1d66c9f282b5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lemmatized Tokens: ['NLP', 'field', 'AI', 'subset', 'AI', '.', 'NLTK', 'powerful', 'tool', 'text', 'analysis', '.', 'NLP', 'play', 'crucial', 'role', 'various', 'task', 'building', 'chatgpt', 'etc', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "#lemmatization reduces a word to its dictionary form, considering its meaning and part of speech (POS). It generally produces more meaningful results than stemming.\n",
    "\n",
    "# Function to apply lemmatization to a list of tokens\n",
    "def lemmatization(tokens):\n",
    "    lemmatizer = WordNetLemmatizer()     #tool that performs lemmatization\n",
    "    return [lemmatizer.lemmatize(word) for word in tokens]   #For each word, it applies the lemmatize method of the WordNetLemmatizer object to reduce the word to its base or dictionary form\n",
    "\n",
    "# Example Usage:\n",
    "lemmatized_tokens = lemmatization(filtered_tokens)\n",
    "print(\"Lemmatized Tokens:\", lemmatized_tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 51,
     "status": "ok",
     "timestamp": 1740091396770,
     "user": {
      "displayName": "dipali porje",
      "userId": "15697648671118973123"
     },
     "user_tz": -330
    },
    "id": "ONigpXuGh0lC",
    "outputId": "165654d2-e0c4-46f2-d064-b336a28f6cec"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "TF-IDF Representation:\n",
      "   analysis  analytics       and  document  essential       for        in  \\\n",
      "0  0.324124   0.000000  0.000000  0.000000   0.000000  0.426184  0.000000   \n",
      "1  0.270118   0.000000  0.355173  0.000000   0.000000  0.000000  0.000000   \n",
      "2  0.000000   0.410747  0.000000  0.410747   0.410747  0.000000  0.410747   \n",
      "\n",
      "   involves        is  lemmatization      like      nltk  powerful  \\\n",
      "0  0.000000  0.324124       0.000000  0.000000  0.426184  0.426184   \n",
      "1  0.355173  0.000000       0.355173  0.355173  0.000000  0.000000   \n",
      "2  0.000000  0.312384       0.000000  0.000000  0.000000  0.000000   \n",
      "\n",
      "   preprocessing  stemming  techniques      text  tokenization      tool  \n",
      "0       0.000000  0.000000    0.000000  0.251711      0.000000  0.426184  \n",
      "1       0.000000  0.355173    0.355173  0.209771      0.355173  0.000000  \n",
      "2       0.410747  0.000000    0.000000  0.242594      0.000000  0.000000  \n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import pandas as pd\n",
    "\n",
    "# Function to compute TF-IDF for a list of documents\n",
    "def compute_tfidf(documents):       #takes a list of documents as input.\n",
    "    vectorizer = TfidfVectorizer()      #utility that transforms text data (documents) into numerical vectors using the TF-IDF (Term Frequency-Inverse Document Frequency) technique.\n",
    "    tfidf_matrix = vectorizer.fit_transform(documents)   #fit() method learns the vocabulary (the unique words) and calculates the IDF (Inverse Document Frequency) from the documents.The transform() method then applies this learned information to transform the list of documents into a TF-IDF matrix (a sparse matrix).\n",
    "\n",
    "    # Convert the TF-IDF matrix to a DataFrame for easier visualization\n",
    "    return pd.DataFrame(tfidf_matrix.toarray(), columns=vectorizer.get_feature_names_out())   #toarray() method converts the sparse matrix (returned by fit_transform) into a regular dense numpy array, which is easier to work with.\n",
    "\n",
    "# Example Usage:\n",
    "documents = [\n",
    "    \"NLTK is a powerful tool for text analysis.\",\n",
    "    \"Text analysis involves techniques like tokenization, stemming, and lemmatization.\",\n",
    "    \"Document preprocessing is essential in text analytics.\"\n",
    "]\n",
    "\n",
    "tfidf_df = compute_tfidf(documents)   #DataFrame that contains the TF-IDF scores for each word in the documents.\n",
    "print(\"\\nTF-IDF Representation:\")\n",
    "print(tfidf_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tGVn0eQKh5ES"
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyMusce77vs0SBqgJYDyyhRo",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
